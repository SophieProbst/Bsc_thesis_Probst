{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightkurve as lk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import savgol_filter\n",
    "import os\n",
    "\n",
    "from flare_timestamps import get_timestamps\n",
    "from flare_timestamps import get_rotation_period\n",
    "\n",
    "#these are used for the second half of the dataset (superflares_2.txt) because the dataset had to be split and processed in two parts\n",
    "\n",
    "# from flare_timestamps2 import get_timestamps_2\n",
    "# from flare_timestamps2 import get_rotation_period_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for creating one lightcurve is now applied to create all lightcurves from the superflares.txt file. This is done by looping over all target names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028018 Done\n",
      "2012690 Done\n",
      "2303352 Done\n",
      "2860930 Done\n",
      "3123231 Done\n",
      "3128078 Done\n",
      "3217852 Done\n",
      "3248885 Done\n",
      "3342210 Done\n",
      "3630998 Done\n",
      "3728906 Done\n",
      "3831820 Done\n",
      "3834913 Done\n",
      "3853938 Done\n",
      "3869649 Done\n",
      "3939069 Done\n",
      "4142137 Done\n",
      "4249702 Done\n",
      "4276035 Done\n",
      "4285178 Done\n",
      "4459146 Done\n",
      "4585486 Done\n",
      "4639291 Done\n",
      "4749912 Done\n",
      "4824987 Done\n",
      "4851941 Done\n",
      "4995346 Done\n",
      "5003985 Done\n",
      "5176547 Done\n",
      "5182832 Done\n",
      "5183039 Done\n",
      "5263650 Done\n",
      "5281811 Done\n",
      "5357275 Done\n",
      "5374789 Done\n",
      "5459300 Done\n",
      "5474356 Done\n",
      "5510843 Done\n",
      "5563561 Done\n",
      "5648294 Done\n",
      "5695372 Done\n",
      "5706292 Done\n",
      "5783884 Done\n",
      "5788095 Done\n",
      "5953631 Done\n",
      "5991070 Done\n",
      "6059055 Done\n",
      "6110415 Done\n",
      "6191218 Done\n",
      "6266444 Done\n",
      "6278465 Done\n",
      "6347470 Done\n",
      "6347656 Done\n",
      "6352768 Done\n",
      "6385972 Done\n",
      "6387419 Done\n",
      "6431380 Done\n",
      "6468721 Done\n",
      "6503941 Done\n",
      "6504503 Done\n",
      "6508169 Done\n",
      "6510909 Done\n",
      "6610891 Done\n",
      "6611563 Done\n",
      "6613812 Done\n",
      "6630715 Done\n",
      "6836589 Done\n",
      "6843454 Done\n",
      "6865416 Done\n",
      "6865484 Done\n",
      "6876367 Done\n",
      "6932164 Done\n",
      "6934317 Done\n",
      "6960242 Done\n",
      "7034441 Done\n",
      "7093228 Done\n",
      "7104854 Done\n",
      "7130464 Done\n",
      "7137278 Done\n",
      "7192234 Done\n",
      "7256548 Done\n",
      "7264671 Done\n",
      "7267949 Done\n",
      "7287601 Done\n",
      "7293816 Done\n",
      "7300533 Done\n",
      "7353813 Done\n",
      "7354508 Done\n",
      "7435701 Done\n",
      "7509982 Done\n",
      "7532880 Done\n",
      "7612227 Done\n",
      "7679876 Done\n",
      "7761519 Done\n",
      "7763435 Done\n",
      "7772296 Done\n",
      "7840145 Done\n",
      "7869831 Done\n",
      "7872305 Done\n",
      "7886115 Done\n",
      "7937361 Done\n",
      "7954521 Done\n",
      "8040308 Done\n",
      "8042739 Done\n",
      "8084770 Done\n",
      "8086085 Done\n",
      "8086144 Done\n",
      "8090349 Done\n",
      "8092433 Done\n",
      "8092454 Done\n",
      "8143238 Done\n",
      "8167703 Done\n",
      "8176783 Done\n",
      "8277175 Done\n",
      "8285970 Done\n",
      "8355911 Done\n",
      "8359398 Done\n",
      "8377620 Done\n",
      "8416788 Done\n",
      "8424356 Done\n",
      "8424593 Done\n",
      "8487307 Done\n",
      "8491152 Done\n",
      "8566072 Done\n",
      "8570781 Done\n",
      "8571004 Done\n",
      "8589530 Done\n",
      "8590762 Done\n",
      "8604805 Done\n",
      "8610178 Done\n",
      "8620757 Done\n",
      "8621939 Done\n",
      "8653039 Done\n",
      "8694559 Done\n",
      "8733902 Done\n",
      "8736900 Done\n",
      "8766918 Done\n",
      "8767105 Done\n",
      "8802624 Done\n",
      "8822421 Done\n",
      "8848528 Done\n",
      "8892124 Done\n",
      "8936408 Done\n",
      "8946267 Done\n",
      "8977559 Done\n",
      "9030812 Done\n",
      "9049540 Done\n",
      "9050543 Done\n",
      "9075109 Done\n",
      "9138848 Done\n",
      "9141747 Done\n",
      "9142773 Done\n",
      "9150539 Done\n",
      "9159119 Done\n",
      "9287643 Done\n",
      "9290677 Done\n",
      "9335973 Done\n",
      "9392121 Done\n",
      "9410906 Done\n",
      "9509071 Done\n",
      "9520338 Done\n",
      "9549300 Done\n",
      "9583493 Done\n",
      "9598005 Done\n",
      "9656102 Done\n",
      "9666115 Done\n",
      "9716645 Done\n",
      "9727020 Done\n",
      "9764877 Done\n",
      "9772971 Done\n",
      "9779373 Done\n",
      "9786953 Done\n",
      "9815596 Done\n",
      "9825370 Done\n",
      "9846290 Done\n",
      "9848292 Done\n",
      "9875451 Done\n",
      "9881857 Done\n",
      "9910701 Done\n",
      "9935067 Done\n",
      "9959608 Done\n",
      "9963468 Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recreate the same code as above but for all target names\n",
    "target_names= np.unique(np.loadtxt('superflares_2.txt', dtype='str', usecols=(0)))\n",
    "input_file = 'superflares_2.txt'\n",
    "\n",
    "\n",
    "for target_name in target_names:\n",
    "\n",
    "    #create a subfolder in the 'plots' folder for each target with the target name\n",
    "    if not os.path.exists('plots/' + target_name ):\n",
    "        os.makedirs('plots/' + target_name )\n",
    "\n",
    "    all_timestamps = get_timestamps(target_name, input_file)\n",
    "\n",
    "    try:\n",
    "        search_result = lk.search_lightcurve(f'KIC {target_name}', mission='Kepler', cadence='long')\n",
    "        lc_collection = search_result.download_all()\n",
    "        lc = np.array((lc_collection.stitch().flux) - 1)\n",
    "        t = np.array(lc_collection.stitch().time.value)\n",
    "    except:\n",
    "        print('Target not found')\n",
    "        exit()\n",
    "\n",
    "    timestamps = []\n",
    "    for element in all_timestamps:\n",
    "        element = float(element) + float(2400000)\n",
    "        timestamp = np.round(element - 2454833, 2)\n",
    "        timestamps.append(timestamp)\n",
    "\n",
    "\n",
    "    for element in timestamps:\n",
    "\n",
    "        #plot the lightcurve of the flare for an interval 40 days before and 20 days after the flare\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(t, lc, 'black',  lw=0.3)\n",
    "        plt.xlim(element-40, element+20)\n",
    "        plt.xticks([element-40, element-30, element-20, element-10, element, element+10, element+20], ['-40','-30','-20','-10','0','10','20'])\n",
    "        plt.grid()\n",
    "        plt.title('Kepler Lightcurve for KIC: ' + target_name)\n",
    "        # plt.title('Flare date: ' + str(element) + ' days')\n",
    "        plt.xlabel('Day from peak')\n",
    "        plt.ylabel('Flux (Î”F/F)')\n",
    "        #save the plot directly in the github repository\n",
    "        plt.savefig('plots/' + target_name + '/' + target_name + 'flare' + str(element) + '.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    print(target_name, 'Done')\n",
    "\n",
    "#push the directory to the github repository\n",
    "os.system('git add .')\n",
    "os.system('git commit -m \"automated commit\"')\n",
    "os.system('git push')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second measure for the fluctuation before the flare is the peak to peak value. It can be calculated by finding all the maxima and minima in the lightcurve of the flux. The problem is that the overall lightcurve consists of small fluctuations that should not be counted as extrema. So the lightcurve is smoothed with the savitzky golay filter. It reduces the noise in the data so that the find_peaks function only registers caused by the rotating sgtarspots. The peak to peak values are then calculated by subtracting the minimum from the maximum of the smoothed lightcurve. Finally the mean value of all peak to peak values is calculated. This is only possible for an interval 40 days before the flare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time the hight of the flare or rather the flare intensity can is calculated. This is done by determining the flux value at the time of the flare and subtracting the flux value one data point before the flare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hight=[]\n",
    "peak_peak= []\n",
    "\n",
    "target_names= np.loadtxt('superflares.txt', dtype='str', usecols=0)\n",
    "timestamps= np.loadtxt('superflares.txt', dtype='str', usecols=11)\n",
    "\n",
    "for target_name in target_names:\n",
    "    all_timestamps = get_timestamps(target_name, timestamps)\n",
    "\n",
    "    #search for the lightcurve and download it, if it doesn't exist, skip to the next target\n",
    "    try:\n",
    "        search_result = lk.search_lightcurve(f'KIC {target_name}', mission='Kepler', cadence='long')\n",
    "        lc_collection = search_result.download_all()\n",
    "    except:\n",
    "        print('Lightcurve not found')\n",
    "        continue \n",
    "\n",
    "    timestamps = []\n",
    "    for element in all_timestamps:\n",
    "        element = float(element) + float(2400000)\n",
    "        timestamp = np.round(element - 2454833, 2)\n",
    "        timestamps.append(timestamp)\n",
    "\n",
    "\n",
    "    lc = np.array((lc_collection.stitch().flux) - 1)\n",
    "    t = np.array(lc_collection.stitch().time.value)\n",
    "\n",
    "    #find the rotation period of the target\n",
    "    input_file = 'superflares.txt'\n",
    "    rotation_period = get_rotation_period(target_name, input_file)\n",
    "\n",
    "    flux= []\n",
    "    for element in timestamps:\n",
    "        \n",
    "        #round t to 2 decimals\n",
    "        t = np.round(t, 2)\n",
    "        el = element - 54833\n",
    "\n",
    "        index = np.where(t == element)\n",
    "\n",
    "        \n",
    "        flux.append(lc[index])\n",
    "\n",
    "        #find the flux value one index before the flare\n",
    "        start_index = index[0]\n",
    "\n",
    "        #determine the difference between the flux value of the flare and the flux value one index before the flare\n",
    "        flux_diff = (lc[index] - lc[start_index - 1])\n",
    "        hight.append(flux_diff)\n",
    "\n",
    "        yhat = savgol_filter(lc, 101, 3) # window size 51, polynomial order 3\n",
    "\n",
    "        #find the peaks in the lightcurve\n",
    "        #calculate the mean time difference between two data points\n",
    "        mean_time_diff = np.mean(np.diff(t))\n",
    "        peaks, _ = find_peaks(yhat, height=0, distance=rotation_period/mean_time_diff)\n",
    "        minima = find_peaks(-yhat, height=0, distance=rotation_period/mean_time_diff)\n",
    "\n",
    "        #only take the peaks 40 days before and after the flare\n",
    "        peaks = peaks[(peaks > el - 40)]\n",
    "        minima = minima[0][(minima[0] > el - 40)]\n",
    "\n",
    "        #match the number of peaks with the number of minima\n",
    "        if len(peaks) > len(minima):\n",
    "            peaks = peaks[0:len(minima)]\n",
    "        elif len(minima) > len(peaks):\n",
    "            minima = minima[0:len(peaks)]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "        differnce = np.mean(yhat[peaks] - yhat[minima])\n",
    "        peak_peak.append(differnce)\n",
    "\n",
    "    print(target_name, 'Done')\n",
    "\n",
    " #save the created array as a txt file\n",
    "# np.savetxt('peak_peak.txt', peak_peak, fmt='%s')\n",
    "# np.savetxt('hight.txt', hight, fmt='%s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The superflares are then separated according to their rotation period which is in the 6. column of the superflares.txt file. The rotation periods are then matched to the peak to peak value as well as the bolometric energy (used later fro different plots). Then 4 different arrays are created for each rotation period interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "superflares= np.loadtxt('superflares.txt', usecols= (0,6,13))\n",
    "superflares_2= np.loadtxt('superflares_2.txt', usecols= (0,6,13))\n",
    "\n",
    "superflares= np.concatenate((superflares, superflares_2))\n",
    "\n",
    "peak_peak= np.loadtxt('peak_peak.txt')\n",
    "peak_peak_2= np.loadtxt('peak_peak_2.txt')\n",
    "\n",
    "peak_peak= np.concatenate((peak_peak, peak_peak_2))\n",
    "\n",
    "#match the length of peak_peak to superflares\n",
    "peak_peak= peak_peak[:len(superflares)]\n",
    "\n",
    "#match the length of superflares to peak_peak\n",
    "superflares= superflares[:len(peak_peak)]\n",
    "\n",
    "all= np.column_stack((superflares, peak_peak))\n",
    "\n",
    "#find all rows where the value in the second colum is under 5 or equal to 5\n",
    "all5= all[all[:,1] <= 5]\n",
    "\n",
    "#repeat the same for values between 6 and 10 including 10\n",
    "all10= all[(all[:,1] > 5) & (all[:,1] <= 10)]\n",
    "\n",
    "#repeat the same for values between 11 and 20 including 20\n",
    "all20= all[(all[:,1] > 10) & (all[:,1] <= 20)]\n",
    "\n",
    "#repeat the same for values between 21 and 40 including 40\n",
    "all40= all[(all[:,1] > 20) & (all[:,1] <= 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical significance of the data can now be calculated. This is done same as before by calculating the correlation coefficients and the p values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 days: 0.08025358052288556\n",
      "10 days: 0.097141978161468\n",
      "20 days: -0.062394453960607436\n",
      "40 days: -0.07018489105126458\n"
     ]
    }
   ],
   "source": [
    "#remove all nan values from the data\n",
    "all5=all5[~np.isnan(all5).any(axis=1)]\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(all5[:,2],all5[:,3])\n",
    "print('5 days:', r_value)\n",
    "\n",
    "all10=all10[~np.isnan(all10).any(axis=1)]\n",
    "slope10, intercept10, r_value10, p_value10, std_err10 = stats.linregress(all10[:,2],all10[:,3])\n",
    "print('10 days:',r_value10)\n",
    "\n",
    "all20=all20[~np.isnan(all20).any(axis=1)]\n",
    "slope20, intercept20, r_value20, p_value20, std_err20 = stats.linregress(all20[:,2],all20[:,3])\n",
    "print('20 days:',r_value20)\n",
    "\n",
    "all40=all40[~np.isnan(all40).any(axis=1)]\n",
    "slope40, intercept40, r_value40, p_value40, std_err40 = stats.linregress(all40[:,2],all40[:,3])\n",
    "print('40 days:',r_value40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('all5.txt', all5, fmt='%s')\n",
    "np.savetxt('all10.txt', all10, fmt='%s')\n",
    "np.savetxt('all20.txt', all20, fmt='%s')\n",
    "np.savetxt('all40.txt', all40, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(all5[:,2],all5[:,3])\n",
    "slope10, intercept10, r_value10, p_value10, std_err10 = stats.linregress(all10[:,2],all10[:,3])\n",
    "slope20, intercept20, r_value20, p_value20, std_err20 = stats.linregress(all20[:,2],all20[:,3])\n",
    "slope40, intercept40, r_value40, p_value40, std_err40 = stats.linregress(all40[:,2],all40[:,3])\n",
    "\n",
    "#with the r_vlaue claculate the standard error\n",
    "r_value_std= np.sqrt((1-r_value**2)/(len(all5)-2))\n",
    "r_value_std10= np.sqrt((1-r_value10**2)/(len(all10)-2))\n",
    "r_value_std20= np.sqrt((1-r_value20**2)/(len(all20)-2))\n",
    "r_value_std40= np.sqrt((1-r_value40**2)/(len(all40)-2))\n",
    "\n",
    "#calculate the z-score\n",
    "z_score= r_value/r_value_std\n",
    "z_score10= r_value10/r_value_std10\n",
    "z_score20= r_value20/r_value_std20\n",
    "z_score40= r_value40/r_value_std40\n",
    "\n",
    "#calculate the p-value\n",
    "p_value= 2*(1-stats.norm.cdf(np.abs(z_score)))\n",
    "p_value10= 2*(1-stats.norm.cdf(np.abs(z_score10)))\n",
    "p_value20= 2*(1-stats.norm.cdf(np.abs(z_score20)))\n",
    "p_value40= 2*(1-stats.norm.cdf(np.abs(z_score40)))\n",
    "\n",
    "print('5 days:',  p_value)\n",
    "print('10 days:', p_value10)\n",
    "print('20 days:',  p_value20)\n",
    "print('40 days:', p_value40)\n",
    "\n",
    "print(r_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
