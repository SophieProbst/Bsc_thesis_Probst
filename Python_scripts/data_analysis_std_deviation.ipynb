{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightkurve as lk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import savgol_filter\n",
    "import os\n",
    "\n",
    "from flare_timestamps import get_timestamps\n",
    "from flare_timestamps import get_rotation_period\n",
    "\n",
    "#these are used for the second half of the dataset (superflares_2.txt) because the dataset had to be split and processed in two parts\n",
    "\n",
    "from flare_timestamps2 import get_timestamps_2\n",
    "from flare_timestamps2 import get_rotation_period_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the standard deviation of the flux in different time intervals before the flare, the time intervals have to be defined. For that the amount of datapoints in each interval have to be calculated first. Then the flux values for these datapoints can be put into separate arrays and then the standard deviation can be calculated for each array separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new folders for the different standard deviations\n",
    "os.mkdir('std_1d')\n",
    "os.mkdir('std_5d')\n",
    "os.mkdir('std_10d')\n",
    "os.mkdir('std_20d')\n",
    "os.mkdir('std_40d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreate the same code as above but for all target names\n",
    "target_names= np.unique(np.loadtxt('superflares.txt', dtype='str', usecols=(0)))\n",
    "input_file = 'superflares.txt'\n",
    "\n",
    "\n",
    "for target_name in target_names:\n",
    "    all_timestamps = get_timestamps(target_name, input_file)\n",
    "    rotation_period = get_rotation_period(target_name, input_file)\n",
    "\n",
    "    try:\n",
    "        search_result = lk.search_lightcurve(f'KIC {target_name}', mission='Kepler', cadence='long')\n",
    "        lc_collection = search_result.download_all()\n",
    "        lc = np.array((lc_collection.stitch().flux) - 1)\n",
    "        t = np.array(lc_collection.stitch().time.value)\n",
    "    except:\n",
    "        print('Target not found')\n",
    "        exit()\n",
    "\n",
    "    timestamps = []\n",
    "    for element in all_timestamps:\n",
    "        element = float(element) + float(2400000)\n",
    "        timestamp = np.round(element - 2454833, 2)\n",
    "        timestamps.append(timestamp)\n",
    "\n",
    "    std_1d= []\n",
    "    std_5d= []\n",
    "    std_10d= []\n",
    "    std_20d= []\n",
    "    std_40d= []\n",
    "    for element in timestamps:\n",
    "\n",
    "\n",
    "        #round t to 2 decimals\n",
    "        t = np.round(t, 2)\n",
    "        el = int(element - 54833)\n",
    "        index = np.where(t == element)\n",
    "        # # print(lc[index])\n",
    "\n",
    "        # flux.append(lc[index])\n",
    "\n",
    "        #find the flux value one index before the flare\n",
    "        start_index = index[0]\n",
    "        # print(lc[start_index - 1])\n",
    "\n",
    "\n",
    "        #calculate the mean time difference between two data points\n",
    "        mean_time_diff = np.mean(np.diff(t))\n",
    "\n",
    "        #from the mean time difference, calculate how many data points are in 1, 5, 10, 20 and 40 days\n",
    "        one_day = int(1/mean_time_diff)\n",
    "        five_days = int(5/mean_time_diff)\n",
    "        ten_days = int(10/mean_time_diff)\n",
    "        twenty_days = int(20/mean_time_diff)\n",
    "        fourty_days = int(40/mean_time_diff)\n",
    "\n",
    "        #make a separate array saving only the flux values one day before the flare\n",
    "        lc_1d_before = lc[el- one_day:el]\n",
    "        lc_5d_before = lc[el-five_days:el]\n",
    "        lc_10d_before = lc[el-ten_days:el]\n",
    "        lc_20d_before = lc[el-twenty_days:el]\n",
    "        lc_40d_before = lc[el-fourty_days:el]\n",
    "\n",
    "        #erase all nan values from the arrays\n",
    "        lc_1d_before = lc_1d_before[~np.isnan(lc_1d_before)]\n",
    "        lc_5d_before = lc_5d_before[~np.isnan(lc_5d_before)]\n",
    "        lc_10d_before = lc_10d_before[~np.isnan(lc_10d_before)]\n",
    "        lc_20d_before = lc_20d_before[~np.isnan(lc_20d_before)]\n",
    "        lc_40d_before = lc_40d_before[~np.isnan(lc_40d_before)]\n",
    "\n",
    "        #calculate the standard deviation of the arrays above\n",
    "        std_1d_before = np.std(lc_1d_before)\n",
    "        std_5d_before = np.std(lc_5d_before)\n",
    "        std_10d_before = np.std(lc_10d_before)\n",
    "        std_20d_before = np.std(lc_20d_before)\n",
    "        std_40d_before = np.std(lc_40d_before)\n",
    "\n",
    "\n",
    "        std_1d.append(std_1d_before)\n",
    "        std_5d.append(std_5d_before)\n",
    "        std_10d.append(std_10d_before)\n",
    "        std_20d.append(std_20d_before)\n",
    "        std_40d.append(std_40d_before)\n",
    "\n",
    "    #for each target name, write the standard deviations in separate text files and save them in the std_1d, std_5d, std_10d, std_20d and std_40d folders\n",
    "    np.savetxt(f'std_1d/std_1d_{target_name}.txt', std_1d, fmt='%s')\n",
    "    np.savetxt(f'std_5d/std_5d_{target_name}.txt', std_5d, fmt='%s')\n",
    "    np.savetxt(f'std_10d/std_10d_{target_name}.txt', std_10d, fmt='%s')\n",
    "    np.savetxt(f'std_20d/std_20d_{target_name}.txt', std_20d, fmt='%s')\n",
    "    np.savetxt(f'std_40d/std_40d_{target_name}.txt', std_40d, fmt='%s')\n",
    "    print(target_name, 'done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create suitable arrays for the standard deviation and bolometric energy so that they can be plotted against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\AppData\\Local\\Temp\\ipykernel_13728\\3491390316.py:2: UserWarning: loadtxt: input contained no data: \"std_1d_2.txt\"\n",
      "  std_1d_2=np.loadtxt('std_1d_2.txt')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "std_1d=np.loadtxt('std_1d.txt')\n",
    "std_1d_2=np.loadtxt('std_1d_2.txt')\n",
    "std_5d=np.loadtxt('std_5d.txt')\n",
    "std_5d_2= np.loadtxt('std_5d_2.txt')\n",
    "std_10d=np.loadtxt('std_10d.txt')\n",
    "std_10d_2=np.loadtxt('std_10d_2.txt')\n",
    "std_20d=np.loadtxt('std_20d.txt')\n",
    "std_20d_2=np.loadtxt('std_20d_2.txt')\n",
    "std_40d=np.loadtxt('std_40d.txt')\n",
    "std_40d_2=np.loadtxt('std_40d_2.txt')\n",
    "bolom= np.loadtxt('superflares.txt', usecols=13)\n",
    "bolom1= np.loadtxt('superflares.txt', usecols=13)\n",
    "bolom5= np.loadtxt('superflares.txt', usecols=13)\n",
    "bolom10= np.loadtxt('superflares.txt', usecols=13)\n",
    "bolom20= np.loadtxt('superflares.txt', usecols=13)\n",
    "bolom40= np.loadtxt('superflares.txt', usecols=13)\n",
    "\n",
    "#match the number of bolometric energies with the number of standard deviations, then combine them in one array with two columns and then remove the rows with nan values\n",
    "if len(bolom1) > len(std_1d):\n",
    "    bolom1 = bolom1[0:len(std_1d)]\n",
    "elif len(std_1d) > len(bolom1):\n",
    "    std_1d = std_1d[0:len(bolom1)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(bolom5) > len(std_5d):\n",
    "    bolom5 = bolom5[0:len(std_5d)]\n",
    "elif len(std_5d) > len(bolom5):\n",
    "    std_5d = std_5d[0:len(bolom5)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(bolom) > len(std_5d_2):\n",
    "    bolom5_2 = bolom[0:len(std_5d_2)]\n",
    "elif len(std_5d_2) > len(bolom):\n",
    "    std_5d_2 = std_5d_2[0:len(bolom)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#repeat the same for std_10d_2, std_20d_2 and std_40d_2\n",
    "if len(bolom) > len(std_10d_2):\n",
    "    bolom10_2 = bolom[0:len(std_10d_2)]\n",
    "elif len(std_10d_2) > len(bolom):\n",
    "    std_10d_2 = std_10d_2[0:len(bolom)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(bolom) > len(std_20d_2):\n",
    "    bolom20_2 = bolom[0:len(std_20d_2)]\n",
    "elif len(std_20d_2) > len(bolom):\n",
    "    std_20d_2 = std_20d_2[0:len(bolom)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(bolom) > len(std_40d_2):\n",
    "    bolom40_2 = bolom[0:len(std_40d_2)]\n",
    "elif len(std_40d_2) > len(bolom):\n",
    "    std_40d_2 = std_40d_2[0:len(bolom)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(bolom10) > len(std_10d):\n",
    "    bolom10 = bolom10[0:len(std_10d)]\n",
    "elif len(std_10d) > len(bolom10):\n",
    "    std_10d = std_10d[0:len(bolom10)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(bolom20) > len(std_20d):\n",
    "    bolom20 = bolom20[0:len(std_20d)]\n",
    "elif len(std_20d) > len(bolom20):\n",
    "    std_20d = std_20d[0:len(bolom20)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(bolom40) > len(std_40d):\n",
    "    bolom40 = bolom40[0:len(std_40d)]\n",
    "elif len(std_40d) > len(bolom40):\n",
    "    std_40d = std_40d[0:len(bolom40)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "combined1 = np.column_stack((bolom1, std_1d))\n",
    "combined5 = np.column_stack((bolom5, std_5d))\n",
    "combined10 = np.column_stack((bolom10, std_10d))\n",
    "combined20 = np.column_stack((bolom20, std_20d))\n",
    "combined40 = np.column_stack((bolom40, std_40d))\n",
    "combined5_2 = np.column_stack((bolom5_2, std_5d_2))\n",
    "combined10_2 = np.column_stack((bolom10_2, std_10d_2))\n",
    "combined20_2 = np.column_stack((bolom20_2, std_20d_2))\n",
    "combined40_2 = np.column_stack((bolom40_2, std_40d_2))\n",
    "\n",
    "combined1 = combined1[~np.isnan(combined1).any(axis=1)]\n",
    "combined5 = combined5[~np.isnan(combined5).any(axis=1)]\n",
    "combined10 = combined10[~np.isnan(combined10).any(axis=1)]\n",
    "combined20 = combined20[~np.isnan(combined20).any(axis=1)]\n",
    "combined40 = combined40[~np.isnan(combined40).any(axis=1)]\n",
    "combined5_2 = combined5_2[~np.isnan(combined5_2).any(axis=1)]\n",
    "combined10_2 = combined10_2[~np.isnan(combined10_2).any(axis=1)]\n",
    "combined20_2 = combined20_2[~np.isnan(combined20_2).any(axis=1)]\n",
    "combined40_2 = combined40_2[~np.isnan(combined40_2).any(axis=1)]\n",
    "\n",
    "#combine the two stacks combined5 and combined5_2\n",
    "combined5 = np.concatenate((combined5, combined5_2), axis=0)\n",
    "combined10 = np.concatenate((combined10, combined10_2), axis=0)\n",
    "combined20 = np.concatenate((combined20, combined20_2), axis=0)\n",
    "combined40 = np.concatenate((combined40, combined40_2), axis=0)\n",
    "\n",
    "#save the combined arrays in separate text files\n",
    "np.savetxt('combined1_std.txt', combined1, fmt='%s')\n",
    "np.savetxt('combined5_std.txt', combined5, fmt='%s')\n",
    "np.savetxt('combined10_std.txt', combined10, fmt='%s')\n",
    "np.savetxt('combined20_std.txt', combined20, fmt='%s')\n",
    "np.savetxt('combined40_std.txt', combined40, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all nan values and zeros from the arrays\n",
    "combined1 = combined1[~np.isnan(combined1).any(axis=1)]\n",
    "combined5 = combined5[~np.isnan(combined5).any(axis=1)]\n",
    "combined1 = combined1[~(combined1 == 0).any(axis=1)]\n",
    "combined5 = combined5[~(combined5 == 0).any(axis=1)]\n",
    "combined10 = combined10[~np.isnan(combined10).any(axis=1)]\n",
    "combined20 = combined20[~np.isnan(combined20).any(axis=1)]\n",
    "combined40 = combined40[~np.isnan(combined40).any(axis=1)]\n",
    "combined10= combined10[~(combined10 == 0).any(axis=1)]\n",
    "combined20 = combined20[~(combined20 == 0).any(axis=1)]\n",
    "combined40 = combined40[~(combined40 == 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate all necessary statistical values such as the correlation coefficient, and the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_value for 1 day: 0.4182093486369201\n",
      "r_value for 5 days: 0.23114831095666674\n",
      "r_value for 10 days: 0.22544629508470077\n",
      "r_value for 20 days: 0.2215112890648914\n",
      "r_value for 40 days: 0.2343407302153954\n",
      "Standard error for 1 day: 0.023707759521323417\n",
      "Standard error for 5 days: 0.021870241367539168\n",
      "Standard error for 10 days: 0.021900297390379315\n",
      "Standard error for 20 days: 0.02192057818032827\n",
      "Standard error for 40 days: 0.021853067402714598\n",
      "p-value for 1 day: 6.651371595881552e-09\n",
      "p-value for 5 days: 1.9467546964419942e-25\n",
      "p-value for 10 days: 2.3363941312945964e-09\n",
      "p-value for 20 days: 1.116294445497686e-09\n",
      "p-value for 40 days: 1.1800677280326006e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#calculate the linear regressions\n",
    "slope1, intercept1, r_value1, p_value1, std_err1 = stats.linregress(combined1[:,0], combined1[:,1])\n",
    "slope5, intercept5, r_value5, p_value5, std_err5 = stats.linregress(combined5[:,0], combined5[:,1])\n",
    "slope10, intercept10, r_value10, p_value10, std_err10 = stats.linregress(combined10[:,0], combined10[:,1])\n",
    "slope20, intercept20, r_value20, p_value20, std_err20 = stats.linregress(combined20[:,0], combined20[:,1])\n",
    "slope40, intercept40, r_value40, p_value40, std_err40 = stats.linregress(combined40[:,0], combined40[:,1])\n",
    "\n",
    "#print the r_values\n",
    "print('r_value for 1 day:', r_value1)\n",
    "print('r_value for 5 days:', r_value5)\n",
    "print('r_value for 10 days:', r_value10)\n",
    "print('r_value for 20 days:', r_value20)\n",
    "print('r_value for 40 days:', r_value40)\n",
    "\n",
    "\n",
    "#calculate the standard errors of the r_values\n",
    "std_err1 = np.sqrt((1 - r_value1**2)/(len(combined1)-2))\n",
    "std_err5 = np.sqrt((1 - r_value5**2)/(len(combined5)-2))\n",
    "std_err10 = np.sqrt((1 - r_value10**2)/(len(combined10)-2))\n",
    "std_err20 = np.sqrt((1 - r_value20**2)/(len(combined20)-2))\n",
    "std_err40 = np.sqrt((1 - r_value40**2)/(len(combined40)-2))\n",
    "\n",
    "#print the standard errors\n",
    "print('Standard error for 1 day:', std_err1)\n",
    "print('Standard error for 5 days:', std_err5)\n",
    "print('Standard error for 10 days:', std_err10)\n",
    "print('Standard error for 20 days:', std_err20)\n",
    "print('Standard error for 40 days:', std_err40)\n",
    "\n",
    "\n",
    "#calculate the the test statistic (z-score) and then the p-value\n",
    "z_score1 = (r_value1 - r_value5)/np.sqrt(std_err1**2 + std_err5**2)\n",
    "p_value1 = stats.norm.sf(abs(z_score1))*2\n",
    "\n",
    "z_score40= (r_value1-r_value40)/np.sqrt(std_err1**2 + std_err40**2)\n",
    "p_value40 = stats.norm.sf(abs(z_score40))*2\n",
    "\n",
    "z_score20= (r_value1-r_value20)/np.sqrt(std_err1**2 + std_err20**2)\n",
    "p_value20 = stats.norm.sf(abs(z_score20))*2\n",
    "\n",
    "z_score10= (r_value1-r_value10)/np.sqrt(std_err1**2 + std_err10**2)\n",
    "p_value10 = stats.norm.sf(abs(z_score10))*2\n",
    "\n",
    "print ('p-value for 1 day:', p_value1)\n",
    "print ('p-value for 5 days:', p_value5)\n",
    "print ('p-value for 10 days:', p_value10)\n",
    "print ('p-value for 20 days:', p_value20)\n",
    "print ('p-value for 40 days:', p_value40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical test can also be performed for the number of occurances (the histogram) of the standard deviations. Here only the standard deviations are considered without the bolometric energies of the flares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006672579817292518\n",
      "0.011453248188576477\n",
      "0.011822796832715799\n",
      "0.012031210126900555\n",
      "0.011947132496945989\n",
      "0.00417043575\n",
      "0.009824857\n",
      "0.01045806\n",
      "0.010868426\n",
      "0.01140157\n",
      "0.0070828930446855285\n",
      "0.009498201513793554\n",
      "0.009372056016293463\n",
      "0.00899790711200133\n",
      "0.008186938900186602\n",
      "[-0.00888716  1.18346516  1.37057035 ... -0.89213789 -0.88220901\n",
      " -0.74810425]\n",
      "[-0.50183271  0.31009837  0.35874463 ... -1.02466294 -1.01375378\n",
      " -1.00552282]\n",
      "[-0.51299931  0.07676674  0.12199235 ... -1.07300371 -1.07156247\n",
      " -1.06279306]\n",
      "[-0.12530137 -0.18329853 -0.15719212 ... -1.15339974 -1.15174139\n",
      " -1.13376552]\n",
      "[ 0.68775895  0.21471578  0.20469134 ... -1.27493766 -1.27146459\n",
      " -1.26107058]\n"
     ]
    }
   ],
   "source": [
    "#calculate the mean in each hisogram\n",
    "\n",
    "mean1= np.mean(combined1[:,1])\n",
    "print(mean1)\n",
    "\n",
    "mean5= np.mean(combined5[:,1])\n",
    "print(mean5)\n",
    "\n",
    "mean10= np.mean(combined10[:,1])\n",
    "print(mean10)\n",
    "\n",
    "mean20= np.mean(combined20[:,1])\n",
    "print(mean20)\n",
    "\n",
    "mean40= np.mean(combined40[:,1])\n",
    "print(mean40)\n",
    "\n",
    "median1= np.median(combined1[:,1])\n",
    "print(median1)\n",
    "median5= np.median(combined5[:,1])\n",
    "print(median5)\n",
    "median10= np.median(combined10[:,1])\n",
    "print(median10)\n",
    "median20= np.median(combined20[:,1])\n",
    "print(median20)\n",
    "median40= np.median(combined40[:,1])\n",
    "print(median40)\n",
    "\n",
    "std1= np.std(combined1[:,1])\n",
    "print(std1)\n",
    "std5= np.std(combined5[:,1])\n",
    "print(std5)\n",
    "std10= np.std(combined10[:,1])\n",
    "print(std10)\n",
    "std20= np.std(combined20[:,1])\n",
    "print(std20)\n",
    "std40= np.std(combined40[:,1])\n",
    "print(std40)\n",
    "\n",
    "z_score1= (combined1[:,1] - mean1)/std1\n",
    "print(z_score1)\n",
    "z_score5= (combined5[:,1] - mean5)/std5\n",
    "print(z_score5)\n",
    "z_score10= (combined10[:,1] - mean10)/std10\n",
    "print(z_score10)\n",
    "z_score20= (combined20[:,1] - mean20)/std20\n",
    "print(z_score20)\n",
    "z_score40= (combined40[:,1] - mean40)/std40\n",
    "print(z_score40)\n",
    "\n",
    "\n",
    "\n",
    "#create new combined arrays now with the z_scores\n",
    "combined1 = np.column_stack((combined1[:,0], z_score1))\n",
    "combined5 = np.column_stack((combined5[:,0], z_score5))\n",
    "combined10 = np.column_stack((combined10[:,0], z_score10))\n",
    "combined20 = np.column_stack((combined20[:,0], z_score20))\n",
    "combined40 = np.column_stack((combined40[:,0], z_score40))\n",
    "\n",
    "#sort the arrays by the z_score starting with the highest z_score\n",
    "combined1 = combined1[combined1[:,1].argsort()[::-1]]\n",
    "combined5 = combined5[combined5[:,1].argsort()[::-1]]\n",
    "combined10 = combined10[combined10[:,1].argsort()[::-1]]\n",
    "combined20 = combined20[combined20[:,1].argsort()[::-1]]\n",
    "combined40 = combined40[combined40[:,1].argsort()[::-1]]\n",
    "\n",
    "#erase the rows with z_score over 3\n",
    "combined1 = combined1[combined1[:,1] < 2]\n",
    "combined5 = combined5[combined5[:,1] < 2]\n",
    "combined10 = combined10[combined10[:,1] < 2]\n",
    "combined20 = combined20[combined20[:,1] < 2]\n",
    "combined40 = combined40[combined40[:,1] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for combined1 and combined5: 0.7309358238477365\n",
      "p-value for combined5 and combined10: 0.8775395949610205\n",
      "p-value for combined10 and combined20: 0.6587949614782086\n",
      "p-value for combined20 and combined40: 0.15942236366078874\n",
      "Statistics=0.859, p=0.000\n",
      "Statistics=0.925, p=0.000\n",
      "Statistics=0.934, p=0.000\n",
      "Statistics=0.945, p=0.000\n",
      "Statistics=0.953, p=0.000\n",
      "Statistics=8.746, p=0.003\n",
      "Statistics=0.215, p=0.643\n",
      "Statistics=4.584, p=0.032\n",
      "Statistics=16.002, p=0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#perform a hypothesis tests such as an Analysis of Variance between all the combined arrays separately, meaning in between combined1 and combined5, then combined5 and combined10, then combined10 and combined20 and finally combined20 and combined40\n",
    "\n",
    "#perform an Analysis of Variance between combined1 and combined5\n",
    "f_value, p_value = stats.f_oneway(combined1[:,1], combined5[:,1])\n",
    "print('p-value for combined1 and combined5:', p_value)\n",
    "\n",
    "#perform an Analysis of Variance between combined5 and combined10\n",
    "f_value, p_value = stats.f_oneway(combined5[:,1], combined10[:,1])\n",
    "print('p-value for combined5 and combined10:', p_value)\n",
    "\n",
    "#perform an Analysis of Variance between combined10 and combined20\n",
    "f_value, p_value = stats.f_oneway(combined10[:,1], combined20[:,1])\n",
    "print('p-value for combined10 and combined20:', p_value)\n",
    "\n",
    "#perform an Analysis of Variance between combined20 and combined40\n",
    "f_value, p_value = stats.f_oneway(combined20[:,1], combined40[:,1])\n",
    "print('p-value for combined20 and combined40:', p_value)\n",
    "\n",
    "#now perform a Tukey's HSD (Honestly Significant Difference) test between all the arrays\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "#perform normality tests (e.g., Shapiro-Wilk) and Levene's test to see if the requirements are met for the HSD test\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "#perform a Shapiro-Wilk test for normality\n",
    "stat, p = shapiro(combined1[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "#perform a Shapiro-Wilk test for normality\n",
    "stat, p = shapiro(combined5[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "#perform a Shapiro-Wilk test for normality\n",
    "stat, p = shapiro(combined10[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "#perform a Shapiro-Wilk test for normality\n",
    "stat, p = shapiro(combined20[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "#perform a Shapiro-Wilk test for normality\n",
    "stat, p = shapiro(combined40[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "#perform a Levene's test for homogeneity of variances\n",
    "from scipy.stats import levene\n",
    "\n",
    "#perform a Levene's test for homogeneity of variances\n",
    "stat, p = levene(combined1[:,1], combined5[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "stat, p = levene(combined5[:,1], combined10[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "stat, p = levene(combined10[:,1], combined20[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "stat, p = levene(combined20[:,1], combined40[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics=1.583, p=0.208\n",
      "Statistics=0.000, p=0.996\n",
      "Statistics=0.019, p=0.889\n",
      "Statistics=0.681, p=0.409\n"
     ]
    }
   ],
   "source": [
    "#make a pairwwise camparison between the arrays with the Kruskal-Wallis Test\n",
    "stat, p = stats.kruskal(combined1[:,1], combined5[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "stat, p = stats.kruskal(combined5[:,1], combined10[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "stat, p = stats.kruskal(combined10[:,1], combined20[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "stat, p = stats.kruskal(combined20[:,1], combined40[:,1])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis Test Statistic: 2.8620929874352474\n",
      "P-value: 0.5811626149696443\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "# Perform the Kruskal-Wallis test\n",
    "kw_statistic, p_value = kruskal(combined1[:, 1], combined5[:, 1], combined10[:, 1], combined20[:, 1], combined40[:, 1])\n",
    "\n",
    "# Output the results\n",
    "print(f\"Kruskal-Wallis Test Statistic: {kw_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the arrays for the standard deviation and the flare intensity are created. The hight of the flares is calculated in the loop with the peak to peak value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create arrays of the flux hight\n",
    "std_1d= np.loadtxt('std_1d.txt')\n",
    "std_5d=   np.loadtxt('std_5d.txt')\n",
    "std_10d=    np.loadtxt('std_10d.txt')\n",
    "std_20d=   np.loadtxt('std_20d.txt')\n",
    "std_40d=   np.loadtxt('std_40d.txt')\n",
    "\n",
    "flx1 = np.loadtxt('hight_new.txt')\n",
    "flx5 = np.loadtxt('hight_new.txt')\n",
    "flx10 = np.loadtxt('hight_new.txt')\n",
    "flx20 = np.loadtxt('hight_new.txt')\n",
    "flx40 = np.loadtxt('hight_new.txt')\n",
    "\n",
    "#match the size of the flx to the size of the std_1d, std_5d, std_10d, std_20d and std_40d separately\n",
    "if len(flx1) > len(std_1d):\n",
    "    flx1 = flx1[0:len(std_1d)]\n",
    "elif len(std_1d) > len(flx1):\n",
    "    std_1d = std_1d[0:len(flx1)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(flx5) > len(std_5d):\n",
    "    flx5 = flx5[0:len(std_5d)]\n",
    "elif len(std_5d) > len(flx5):\n",
    "    std_5d = std_5d[0:len(flx5)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(flx10) > len(std_10d):\n",
    "    flx10 = flx10[0:len(std_10d)]\n",
    "elif len(std_10d) > len(flx10):\n",
    "    std_10d = std_10d[0:len(flx10)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(flx20) > len(std_20d):\n",
    "    flx20 = flx20[0:len(std_20d)]\n",
    "elif len(std_20d) > len(flx20):\n",
    "    std_20d = std_20d[0:len(flx20)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if len(flx40) > len(std_40d):\n",
    "    flx40 = flx40[0:len(std_40d)]\n",
    "elif len(std_40d) > len(flx40):\n",
    "    std_40d = std_40d[0:len(flx40)]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "#combine the flux hight and the standard deviation arrays\n",
    "combined1 = np.column_stack((flx1, std_1d))\n",
    "combined5 = np.column_stack((flx5, std_5d))\n",
    "combined10 = np.column_stack((flx10, std_10d))\n",
    "combined20 = np.column_stack((flx20, std_20d))\n",
    "combined40 = np.column_stack((flx40, std_40d))\n",
    "\n",
    "\n",
    "#sort the arrays by the flux hight starting with the highest value then remove the first 7 rows as well as all nan value rows\n",
    "combined1 = combined1[combined1[:,0].argsort()[::-1]]\n",
    "combined1 = combined1[7:]\n",
    "combined1 = combined1[~np.isnan(combined1).any(axis=1)]\n",
    "\n",
    "combined5 = combined5[combined5[:,0].argsort()[::-1]]\n",
    "combined5 = combined5[7:]\n",
    "combined5 = combined5[~np.isnan(combined5).any(axis=1)]\n",
    "\n",
    "combined10 = combined10[combined10[:,0].argsort()[::-1]]\n",
    "combined10 = combined10[7:]\n",
    "combined10 = combined10[~np.isnan(combined10).any(axis=1)]\n",
    "\n",
    "combined20 = combined20[combined20[:,0].argsort()[::-1]]\n",
    "combined20 = combined20[7:]\n",
    "combined20 = combined20[~np.isnan(combined20).any(axis=1)]\n",
    "\n",
    "combined40 = combined40[combined40[:,0].argsort()[::-1]]\n",
    "combined40 = combined40[7:]\n",
    "combined40 = combined40[~np.isnan(combined40).any(axis=1)]\n",
    "\n",
    "#save the arrays in txt files\n",
    "np.savetxt('combined1_hight.txt', combined1)\n",
    "np.savetxt('combined5_hight.txt', combined5)\n",
    "np.savetxt('combined10_hight.txt', combined10)\n",
    "np.savetxt('combined20_hight.txt', combined20)\n",
    "np.savetxt('combined40_hight.txt', combined40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient for 1 day: 0.3872360908553505\n",
      "Correlation coefficient for 5 days: 0.32589176788197527\n",
      "Correlation coefficient for 10 days: 0.32368939981268974\n",
      "Correlation coefficient for 20 days: 0.32492838496941545\n",
      "Correlation coefficient for 40 days: 0.3421664463289179\n",
      "p-value for 1 day: 8.506437396945057e-158\n",
      "p-value for 5 days: 1.8959234765587003e-127\n",
      "p-value for 10 days: 2.067775856543004e-126\n",
      "p-value for 20 days: 5.396964117126975e-127\n",
      "p-value for 40 days: 3.187326882666824e-135\n"
     ]
    }
   ],
   "source": [
    "# #conduct a multiple regression analysis\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# #perform the multiple regression analysis\n",
    "# model1 = sm.OLS(combined1[:,1], combined1[:,0]).fit()\n",
    "# model5 = sm.OLS(combined5[:,1], combined5[:,0]).fit()\n",
    "# model10 = sm.OLS(combined10[:,1], combined10[:,0]).fit()\n",
    "# model20 = sm.OLS(combined20[:,1], combined20[:,0]).fit()\n",
    "# model40 = sm.OLS(combined40[:,1], combined40[:,0]).fit()\n",
    "\n",
    "# #calculate the correlation coefficients\n",
    "# print('Correlation coefficient for 1 day:', model1.rsquared)\n",
    "# print('Correlation coefficient for 5 days:', model5.rsquared)\n",
    "# print('Correlation coefficient for 10 days:', model10.rsquared)\n",
    "# print('Correlation coefficient for 20 days:', model20.rsquared)\n",
    "# print('Correlation coefficient for 40 days:', model40.rsquared)\n",
    "\n",
    "# #calculate the p-values\n",
    "# print('p-value for 1 day:', model1.f_pvalue)\n",
    "# print('p-value for 5 days:', model5.f_pvalue)\n",
    "# print('p-value for 10 days:', model10.f_pvalue)\n",
    "# print('p-value for 20 days:', model20.f_pvalue)\n",
    "# print('p-value for 40 days:', model40.f_pvalue)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
