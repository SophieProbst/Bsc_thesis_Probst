{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightkurve as lk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import savgol_filter\n",
    "import os\n",
    "\n",
    "from flare_timestamps import get_timestamps\n",
    "from flare_timestamps import get_rotation_period\n",
    "\n",
    "#these are used for the second half of the dataset (superflares_2.txt) because the dataset had to be split and processed in two parts\n",
    "\n",
    "# from flare_timestamps2 import get_timestamps_2\n",
    "# from flare_timestamps2 import get_rotation_period_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for creating one lightcurve is now applied to create all lightcurves from the superflares.txt file. This is done by looping over all target names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "superflares.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sophi\\Desktop\\backup\\data_analysis.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sophi/Desktop/backup/data_analysis.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#recreate the same code as above but for all target names\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sophi/Desktop/backup/data_analysis.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m target_names\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39;49mloadtxt(\u001b[39m'\u001b[39;49m\u001b[39msuperflares.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstr\u001b[39;49m\u001b[39m'\u001b[39;49m, usecols\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sophi/Desktop/backup/data_analysis.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msuperflares.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sophi/Desktop/backup/data_analysis.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m target_name \u001b[39min\u001b[39;00m target_names:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sophi/Desktop/backup/data_analysis.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sophi/Desktop/backup/data_analysis.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m#create a subfolder in the 'plots' folder for each target with the target name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\npyio.py:1356\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1354\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1356\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[0;32m   1357\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m   1358\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1359\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[0;32m   1361\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\npyio.py:975\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    973\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(fname)\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 975\u001b[0m     fh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[0;32m    976\u001b[0m     \u001b[39mif\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         encoding \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fh, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: superflares.txt not found."
     ]
    }
   ],
   "source": [
    "#recreate the same code as above but for all target names\n",
    "target_names= np.unique(np.loadtxt('superflares.txt', dtype='str', usecols=(0)))\n",
    "input_file = 'superflares.txt'\n",
    "\n",
    "\n",
    "for target_name in target_names:\n",
    "\n",
    "    #create a subfolder in the 'plots' folder for each target with the target name\n",
    "    if not os.path.exists('plots/' + 'KIC' + target_name):\n",
    "        os.makedirs('plots/' + 'KIC' + target_name)\n",
    "\n",
    "    all_timestamps = get_timestamps(target_name, input_file)\n",
    "\n",
    "    try:\n",
    "        search_result = lk.search_lightcurve(f'KIC {target_name}', mission='Kepler', cadence='long')\n",
    "        lc_collection = search_result.download_all()\n",
    "        lc = np.array((lc_collection.stitch().flux) - 1)\n",
    "        t = np.array(lc_collection.stitch().time.value)\n",
    "    except:\n",
    "        print('Target not found')\n",
    "        exit()\n",
    "\n",
    "    timestamps = []\n",
    "    for element in all_timestamps:\n",
    "        element = float(element) + float(2400000)\n",
    "        timestamp = np.round(element - 2454833, 2)\n",
    "        timestamps.append(timestamp)\n",
    "\n",
    "\n",
    "    for element in timestamps:\n",
    "\n",
    "        #plot the lightcurve of the flare for an interval 40 days before and 20 days after the flare\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(t, lc, 'black',  lw=0.3)\n",
    "        plt.xlim(element-40, element+20)\n",
    "        plt.xticks([element-40, element-30, element-20, element-10, element, element+10, element+20], ['-40','-30','-20','-10','0','10','20'])\n",
    "        plt.grid()\n",
    "        plt.title('Kepler Lightcurve for KIC: ' + target_name)\n",
    "        # plt.title('Flare date: ' + str(element) + ' days')\n",
    "        plt.xlabel('Day from peak')\n",
    "        plt.ylabel('Flux (Î”F/F)')\n",
    "        #save the plot directly in the github repository\n",
    "        plt.savefig('plots/' + target_name + '/' + target_name + 'flare' + str(element) + '.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    print(target_name, 'Done')\n",
    "\n",
    "#push the directory to the github repository\n",
    "os.system('git add .')\n",
    "os.system('git commit -m \"automated commit\"')\n",
    "os.system('git push')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second measure for the fluctuation before the flare is the peak to peak value. It can be calculated by finding all the maxima and minima in the lightcurve of the flux. The problem is that the overall lightcurve consists of small fluctuations that should not be counted as extrema. So the lightcurve is smoothed with the savitzky golay filter. It reduces the noise in the data so that the find_peaks function only registers caused by the rotating sgtarspots. The peak to peak values are then calculated by subtracting the minimum from the maximum of the smoothed lightcurve. Finally the mean value of all peak to peak values is calculated. This is only possible for an interval 40 days before the flare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time the hight of the flare or rather the flare intensity can is calculated. This is done by determining the flux value at the time of the flare and subtracting the flux value one data point before the flare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hight=[]\n",
    "peak_peak= []\n",
    "\n",
    "target_names= np.loadtxt('superflares.txt', dtype='str', usecols=0)\n",
    "timestamps= np.loadtxt('superflares.txt', dtype='str', usecols=11)\n",
    "\n",
    "for target_name in target_names:\n",
    "    all_timestamps = get_timestamps(target_name, timestamps)\n",
    "\n",
    "    #search for the lightcurve and download it, if it doesn't exist, skip to the next target\n",
    "    try:\n",
    "        search_result = lk.search_lightcurve(f'KIC {target_name}', mission='Kepler', cadence='long')\n",
    "        lc_collection = search_result.download_all()\n",
    "    except:\n",
    "        print('Lightcurve not found')\n",
    "        continue \n",
    "\n",
    "    timestamps = []\n",
    "    for element in all_timestamps:\n",
    "        element = float(element) + float(2400000)\n",
    "        timestamp = np.round(element - 2454833, 2)\n",
    "        timestamps.append(timestamp)\n",
    "\n",
    "\n",
    "    lc = np.array((lc_collection.stitch().flux) - 1)\n",
    "    t = np.array(lc_collection.stitch().time.value)\n",
    "\n",
    "    #find the rotation period of the target\n",
    "    input_file = 'superflares.txt'\n",
    "    rotation_period = get_rotation_period(target_name, input_file)\n",
    "\n",
    "    flux= []\n",
    "    for element in timestamps:\n",
    "        \n",
    "        #round t to 2 decimals\n",
    "        t = np.round(t, 2)\n",
    "        el = element - 54833\n",
    "\n",
    "        index = np.where(t == element)\n",
    "\n",
    "        \n",
    "        flux.append(lc[index])\n",
    "\n",
    "        #find the flux value one index before the flare\n",
    "        start_index = index[0]\n",
    "\n",
    "        #determine the difference between the flux value of the flare and the flux value one index before the flare\n",
    "        flux_diff = (lc[index] - lc[start_index - 1])\n",
    "        hight.append(flux_diff)\n",
    "\n",
    "        yhat = savgol_filter(lc, 101, 3) # window size 51, polynomial order 3\n",
    "\n",
    "        #find the peaks in the lightcurve\n",
    "        #calculate the mean time difference between two data points\n",
    "        mean_time_diff = np.mean(np.diff(t))\n",
    "        peaks, _ = find_peaks(yhat, height=0, distance=rotation_period/mean_time_diff)\n",
    "        minima = find_peaks(-yhat, height=0, distance=rotation_period/mean_time_diff)\n",
    "\n",
    "        #only take the peaks 40 days before and after the flare\n",
    "        peaks = peaks[(peaks > el - 40)]\n",
    "        minima = minima[0][(minima[0] > el - 40)]\n",
    "\n",
    "        #match the number of peaks with the number of minima\n",
    "        if len(peaks) > len(minima):\n",
    "            peaks = peaks[0:len(minima)]\n",
    "        elif len(minima) > len(peaks):\n",
    "            minima = minima[0:len(peaks)]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "        differnce = np.mean(yhat[peaks] - yhat[minima])\n",
    "        peak_peak.append(differnce)\n",
    "\n",
    "    print(target_name, 'Done')\n",
    "\n",
    " #save the created array as a txt file\n",
    "# np.savetxt('peak_peak.txt', peak_peak, fmt='%s')\n",
    "# np.savetxt('hight.txt', hight, fmt='%s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The superflares are then separated according to their rotation period which is in the 6. column of the superflares.txt file. The rotation periods are then matched to the peak to peak value as well as the bolometric energy (used later fro different plots). Then 4 different arrays are created for each rotation period interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "superflares= np.loadtxt('superflares.txt', usecols= (0,6,13))\n",
    "superflares_2= np.loadtxt('superflares_2.txt', usecols= (0,6,13))\n",
    "\n",
    "superflares= np.concatenate((superflares, superflares_2))\n",
    "\n",
    "peak_peak= np.loadtxt('peak_peak.txt')\n",
    "peak_peak_2= np.loadtxt('peak_peak_2.txt')\n",
    "\n",
    "peak_peak= np.concatenate((peak_peak, peak_peak_2))\n",
    "\n",
    "#match the length of peak_peak to superflares\n",
    "peak_peak= peak_peak[:len(superflares)]\n",
    "\n",
    "#match the length of superflares to peak_peak\n",
    "superflares= superflares[:len(peak_peak)]\n",
    "\n",
    "all= np.column_stack((superflares, peak_peak))\n",
    "\n",
    "#find all rows where the value in the second colum is under 5 or equal to 5\n",
    "all5= all[all[:,1] <= 5]\n",
    "\n",
    "#repeat the same for values between 6 and 10 including 10\n",
    "all10= all[(all[:,1] > 5) & (all[:,1] <= 10)]\n",
    "\n",
    "#repeat the same for values between 11 and 20 including 20\n",
    "all20= all[(all[:,1] > 10) & (all[:,1] <= 20)]\n",
    "\n",
    "#repeat the same for values between 21 and 40 including 40\n",
    "all40= all[(all[:,1] > 20) & (all[:,1] <= 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical significance of the data can now be calculated. This is done same as before by calculating the correlation coefficients and the p values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 days: 0.08025358052288556\n",
      "10 days: 0.097141978161468\n",
      "20 days: -0.062394453960607436\n",
      "40 days: -0.07018489105126458\n"
     ]
    }
   ],
   "source": [
    "#remove all nan values from the data\n",
    "all5=all5[~np.isnan(all5).any(axis=1)]\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(all5[:,2],all5[:,3])\n",
    "print('5 days:', r_value)\n",
    "\n",
    "all10=all10[~np.isnan(all10).any(axis=1)]\n",
    "slope10, intercept10, r_value10, p_value10, std_err10 = stats.linregress(all10[:,2],all10[:,3])\n",
    "print('10 days:',r_value10)\n",
    "\n",
    "all20=all20[~np.isnan(all20).any(axis=1)]\n",
    "slope20, intercept20, r_value20, p_value20, std_err20 = stats.linregress(all20[:,2],all20[:,3])\n",
    "print('20 days:',r_value20)\n",
    "\n",
    "all40=all40[~np.isnan(all40).any(axis=1)]\n",
    "slope40, intercept40, r_value40, p_value40, std_err40 = stats.linregress(all40[:,2],all40[:,3])\n",
    "print('40 days:',r_value40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('all5.txt', all5, fmt='%s')\n",
    "np.savetxt('all10.txt', all10, fmt='%s')\n",
    "np.savetxt('all20.txt', all20, fmt='%s')\n",
    "np.savetxt('all40.txt', all40, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(all5[:,2],all5[:,3])\n",
    "slope10, intercept10, r_value10, p_value10, std_err10 = stats.linregress(all10[:,2],all10[:,3])\n",
    "slope20, intercept20, r_value20, p_value20, std_err20 = stats.linregress(all20[:,2],all20[:,3])\n",
    "slope40, intercept40, r_value40, p_value40, std_err40 = stats.linregress(all40[:,2],all40[:,3])\n",
    "\n",
    "#with the r_vlaue claculate the standard error\n",
    "r_value_std= np.sqrt((1-r_value**2)/(len(all5)-2))\n",
    "r_value_std10= np.sqrt((1-r_value10**2)/(len(all10)-2))\n",
    "r_value_std20= np.sqrt((1-r_value20**2)/(len(all20)-2))\n",
    "r_value_std40= np.sqrt((1-r_value40**2)/(len(all40)-2))\n",
    "\n",
    "#calculate the z-score\n",
    "z_score= r_value/r_value_std\n",
    "z_score10= r_value10/r_value_std10\n",
    "z_score20= r_value20/r_value_std20\n",
    "z_score40= r_value40/r_value_std40\n",
    "\n",
    "#calculate the p-value\n",
    "p_value= 2*(1-stats.norm.cdf(np.abs(z_score)))\n",
    "p_value10= 2*(1-stats.norm.cdf(np.abs(z_score10)))\n",
    "p_value20= 2*(1-stats.norm.cdf(np.abs(z_score20)))\n",
    "p_value40= 2*(1-stats.norm.cdf(np.abs(z_score40)))\n",
    "\n",
    "print('5 days:',  p_value)\n",
    "print('10 days:', p_value10)\n",
    "print('20 days:',  p_value20)\n",
    "print('40 days:', p_value40)\n",
    "\n",
    "print(r_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
